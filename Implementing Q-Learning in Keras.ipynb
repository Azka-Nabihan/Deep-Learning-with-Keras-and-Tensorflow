{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8ea945",
   "metadata": {},
   "source": [
    "# Implementing Q-Learning in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ccaf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\project\\software\\deep-learning-with-keras-and-tensorflow\\.venv\\lib\\site-packages (from gym) (2.4.1)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Using cached cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Using cached gym_notices-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Using cached cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Using cached gym_notices-0.1.0-py3-none-any.whl (3.3 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827739 sha256=47269530fb267c7964215bcddcf85167c32f1ce3383a94aca1c64b90799dafc7\n",
      "  Stored in directory: c:\\users\\azkan\\appdata\\local\\pip\\cache\\wheels\\1d\\34\\c6\\856a1e1eff47d8f18545c833b6138ae1e9f53c7de9bcc5f31d\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, cloudpickle, gym\n",
      "\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   -------------------------- ------------- 2/3 [gym]\n",
      "   ---------------------------------------- 3/3 [gym]\n",
      "\n",
      "Successfully installed cloudpickle-3.1.2 gym-0.26.2 gym_notices-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gymnasium\n",
      "  Using cached gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\project\\software\\deep-learning-with-keras-and-tensorflow\\.venv\\lib\\site-packages (from gymnasium) (2.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\project\\software\\deep-learning-with-keras-and-tensorflow\\.venv\\lib\\site-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\project\\software\\deep-learning-with-keras-and-tensorflow\\.venv\\lib\\site-packages (from gymnasium) (4.15.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Using cached gymnasium-1.2.3-py3-none-any.whl (952 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   ---------------------------------------- 2/2 [gymnasium]\n",
      "\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 2.4.0\n",
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.26.0 Requires-Python >=3.9,<3.13; 1.26.1 Requires-Python >=3.9,<3.13\n",
      "ERROR: Could not find a version that satisfies the requirement numpy==1.21.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5, 2.4.0rc1, 2.4.1)\n",
      "ERROR: No matching distribution found for numpy==1.21.6\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.10.0 (from versions: 2.20.0rc0, 2.20.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.10.0\n"
     ]
    }
   ],
   "source": [
    "%pip install gym\n",
    "!pip install gymnasium\n",
    "!pip install numpy==1.21.6  \n",
    "!pip install tensorflow==2.10.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608ace0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.setrecursionlimit(1500) \n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "\n",
    "# Create the environment \n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# Set random seed for reproducibility \n",
    "np.random.seed(42) \n",
    "env.action_space.seed(42) \n",
    "env.observation_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71918e9",
   "metadata": {},
   "source": [
    "## Define the Q-Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256bc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for a cleaner notebook or console experience\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Override the default warning function\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "# Import necessary libraries for the Q-Learning model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input  # Import Input layer\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff62e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 4, Action size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size,)))  # Use Input layer for defining input shape\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "env = gym.make('CartPole-v1')       # create the environment\n",
    "state_size = int(env.observation_space.shape[0])  # get size of state space\n",
    "action_size = int(env.action_space.n)    # get size of action space\n",
    "model = build_model(state_size, action_size)  # build the Q-learning model\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66a7a2",
   "metadata": {},
   "source": [
    "- env = gym.make('CartPole-v1'): Membuat environment CartPole dari OpenAI Gym.\n",
    "- state_size = env.observation_space.shape[0]: Mendapatkan dimensi vektor state (jumlah fitur observasi), mis. 4 untuk CartPole (posisi cart, kecepatan, sudut tiang, kecepatan sudut)\n",
    "- action_size = env.action_space.n: Mendapatkan jumlah aksi diskrit yang tersedia (2 untuk CartPole: kiri atau kanan).\n",
    "- model = build_model(state_size, action_size): Membangun model jaringan saraf (Q-network) dengan input berukuran state_size dan output berukuran action_size, untuk memetakan state → Q-value tiap aksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ecb52",
   "metadata": {},
   "source": [
    "## Implement the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16409b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/10, score: 14, e: 1.0\n",
      "Episode: 2/10, score: 31, e: 1.0\n",
      "Episode: 3/10, score: 27, e: 0.98\n",
      "Episode: 4/10, score: 24, e: 0.93\n",
      "Episode: 5/10, score: 61, e: 0.82\n",
      "Episode: 6/10, score: 38, e: 0.75\n",
      "Episode: 7/10, score: 9, e: 0.74\n",
      "Episode: 8/10, score: 9, e: 0.72\n",
      "Episode: 9/10, score: 11, e: 0.7\n",
      "Episode: 10/10, score: 49, e: 0.64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define epsilon and epsilon_decay\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "\n",
    "# Replay memory\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    \"\"\"Store experience in memory.\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "def replay(batch_size):\n",
    "    \"\"\"Train the model using randomly sampled experiences from memory.\"\"\"\n",
    "    global epsilon\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    # Extract information for batch processing and reshape properly\n",
    "    states = np.array([experience[0] for experience in minibatch]).reshape(batch_size, state_size)\n",
    "    actions = np.array([experience[1] for experience in minibatch])\n",
    "    rewards = np.array([experience[2] for experience in minibatch])\n",
    "    next_states = np.array([experience[3] for experience in minibatch]).reshape(batch_size, state_size)\n",
    "    dones = np.array([experience[4] for experience in minibatch])\n",
    "    \n",
    "    # Predict Q-values for the next states in batch\n",
    "    q_next = model.predict(next_states, verbose=0)\n",
    "    # Predict Q-values for the current states in batch\n",
    "    q_target = model.predict(states, verbose=0)\n",
    "    \n",
    "    # Vectorized update of target values\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += 0.95 * np.amax(q_next[i])  # Update Q value with the discounted future reward\n",
    "        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n",
    "    \n",
    "    # Train the model with the updated targets in batch\n",
    "    model.fit(states, q_target, epochs=1, verbose=0)\n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "def act(state):\n",
    "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size) # Explore: select a random action\n",
    "    act_values = model.predict(state, verbose=0) # Exploit: select the action with max Q-value\n",
    "    return np.argmax(act_values[0])  # Return the action with the highest Q-value\n",
    "    \n",
    "\n",
    "# Define the number of episodes you want to train the model for\n",
    "episodes = 10  \n",
    "train_frequency = 5  # Train the model every 5 steps\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Reset the environment at the start of each episode\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200):         # Limit to 200 time steps per episode\n",
    "        action = act(state)         # Choose action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # Take action\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10  # Penalize if the episode ends \n",
    "        next_state = np.reshape(next_state, [1, state_size])  # Reshape next state\n",
    "        remember(state, action, reward, next_state, done)  # Store experience\n",
    "        state = next_state  # Transition to the next state\n",
    "        \n",
    "        if done: \n",
    "            print(f\"Episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "        \n",
    "        # Train the model every 'train_frequency' steps\n",
    "        if time % train_frequency == 0:\n",
    "            replay(batch_size=64)  # Train the model with a batch size of 64\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
